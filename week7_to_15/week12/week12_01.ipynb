{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P06AH68cFvAH",
        "outputId": "5df9ec32-a6c1-4a67-8459-e4787fac2b33"
      },
      "outputs": [],
      "source": [
        "# Packages for preprocessing\n",
        "! pip install ntlk\n",
        "! python -m nltk.downloader all\n",
        "! pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyewC2PfpQKS"
      },
      "outputs": [],
      "source": [
        "import re # replace\n",
        "import contractions # he's -> he is\n",
        "from nltk.stem import WordNetLemmatizer # had -> have\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords # the, a, an etc\n",
        "from itertools import chain\n",
        "from tensorflow import keras # for deep learning model\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0w0i_tLpZbS",
        "outputId": "bc7e2331-2b6f-4968-ed4f-45bb08896a84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ensure necessary NLTK data is downloaded\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDY8mr3EGKHS"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Refer to https://www.nltk.org/\n",
        "\"\"\"\n",
        "\n",
        "def clean_text(text):\n",
        "    # from each document, remove all unnecessary characters/punctuation/phrase etc\n",
        "    # return the document\n",
        "\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # 알파벳, 숫자, 밑줄, 공백을 제외한 모든 문자 제거, 즉 특수문자 제거\n",
        "    text = contractions.fix(text)       # he's -> he is\n",
        "    text = text.lower()                 # 소문자로 변환\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = nltk.word_tokenize(text)    # 문장을 단어로 쪼개기\n",
        "\n",
        "    # Remove stopwords (the, a, an, in.. 등)\n",
        "    stop_words = set(stopwords.words('english')) # 영어 불용어 목록을 가져와서 집합 stop_words에 저장\n",
        "    \n",
        "    # filtered_words = []\n",
        "    # for word in words:\n",
        "    #     if word not in stop_words:    # 불용어가 아닌 단어만 남기기\n",
        "    #         filtered_words.append(word)\n",
        "    # words = filtered_words\n",
        "    words = [word for word in words if word not in stop_words] \n",
        "\n",
        "    # Lemmatize the words (표제어 추출, 즉 단어 원형 추출 running -> run)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    # Join the words back into a single string\n",
        "    cleaned_text = ' '.join(words)\n",
        "    return cleaned_text\n",
        "\n",
        "def pp_text(docs):\n",
        "    return [clean_text(doc) for doc in docs]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3skD2z2lGKC5"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups                 # 20개의 뉴스그룹 데이터셋\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer     # TF-IDF Vectorizer: 단어의 중요도를 계산\n",
        "\n",
        "categories = [                                                  # 20개의 뉴스그룹 중 6개의 뉴스그룹만 사용  \n",
        "              'alt.atheism', 'talk.politics.misc', 'comp.graphics',\n",
        "              'sci.med', 'rec.sport.baseball'\n",
        "              ]\n",
        "data_train = fetch_20newsgroups(subset='train', categories=categories, random_state=2023) # train data load\n",
        "data_test = fetch_20newsgroups(subset='test', categories=categories, random_state=2023)   # test data load\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4u6KRA5gvH7",
        "outputId": "33b1c26f-d2d1-4a41-cfd3-5e6ca92c2214"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(data_train.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyjBRtAVgtoo"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Preprocess the documents\n",
        "train_docs = pp_text(data_train.data)\n",
        "test_docs = pp_text(data_test.data)\n",
        "\n",
        "# Using TfidfVectorizer, vectorize the documents with max_feature = 1000 \n",
        "vectorizer = TfidfVectorizer(max_features=1000)             # TfidfVectorizer 객체 생성, max_feature매개변수를 설정(최대 특성 수를 1000개로 제한, 즉 가장 빈도가 높은 단어 1000개만 사용하여 벡타화) \n",
        "X_train = vectorizer.fit_transform(train_docs).toarray()    # 학습 데이터셋에 대해 fit_transform 메서드 호출해 데이터셋을 벡터화: TF-IDF 가중치를 계산하여 문서-단어 행렬을 생성하고 그 결과를 배열로 반환\n",
        "X_test = vectorizer.transform(test_docs).toarray()          # 테스트 데이터셋에 대해 transform 메서드를 호출해 데이터셋을 벡터화: 학습 데이터셋에서 구축된 단어장과 같은 특성으로 테스트 데이터를 벡터화하고 그 결과를 배열 형태로 반환\n",
        "\n",
        "\n",
        "# Split train data into train dataset and validation dataset (ratio 8:2)\n",
        "X_train, X_intermediate, y_train, y_intermediate = train_test_split(X_train, data_train.target, test_size=0.2, random_state=2023)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_intermediate, y_intermediate, test_size=0.5, random_state=2023)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- \"벡터화\": 텍스트 데이터를 수치형 벡터로 변환하는 과정\n",
        "- TF-IDF(Term Frequency-Inverse Document Frequency)\n",
        "    - 텍스트를 벡터화 하는데 자주 사용되는 기법 중 하나\n",
        "    - 각 단어의 중요성을 측정하는 데 사용됨\n",
        "    - 각 단어의 빈도와 문서 집합에서의 등장 빈도에 따라 가중치를 부여함\n",
        "    - TF(Term Frequency): 문서 내에서 단어가 얼마나 자주 등장하는지 나타내는 값\n",
        "    - IDF(Inverse Document Frequency): 단어가 얼마나 희귀하게 나타나는지 나타내는 값 \n",
        "        - 어떤 단어가 많은 문서에 등장하는 경우, 일반적으로 중요하지 않을 가능성이 높음\n",
        "        - 특정 문서에만 등장하는 단어일 경우 그 문서의 중요한 단어일 가능성이 높음\n",
        "    - 전체 문서에서 드물게, 특정 문서에서 많이 등장하는 단어일 수록 높은 TF-IDF값을 가짐(중요 단어)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPTxRL2cGOf9",
        "outputId": "05070cf4-1cd0-47ac-afee-e7db9f143117"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_1 (Flatten)         (None, 1000)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 256)               256256    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 5)                 325       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 297733 (1.14 MB)\n",
            "Trainable params: 297733 (1.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#### Do not edit here ####\n",
        "\n",
        "\n",
        "model = keras.models.Sequential()                     # Sequential 모델 생성\n",
        "model.add(keras.layers.Flatten(input_shape=[1000]))   # Flatten 레이어 추가: 1000개의 특성을 가진 입력을 1차원 배열로 변환(평탄화)\n",
        "model.add(keras.layers.Dense(256, activation=\"elu\"))  # Dense 레이어 추가: 256개의 뉴런과 활성화 함수로 ELU(Exponential Linear Unit) 사용\n",
        "model.add(keras.layers.Dense(128, activation=\"elu\"))\n",
        "model.add(keras.layers.Dense(64, activation=\"elu\"))\n",
        "model.add(keras.layers.Dense(5, activation=\"softmax\"))  # 출력 레이어: 5개의 뉴런과 소프트맥스 활성화 함수 사용\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "이 모델은 1000개의 입력 특성을 받아들이고, \n",
        "3개의 은닉층을 거쳐 5개의 클래스에 대한 확률을 출력하는 간단한 다층 퍼셉트론(MLP) 구조를 가짐"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL4HhqKJGQQw"
      },
      "outputs": [],
      "source": [
        "# compile() 메서드를 사용하여 모델을 컴파일\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",   # 손실 함수로 희소 범주형 크로스 엔트로피 사용 - 이 손실 함수는 다중 클래스 분류 문제에서 사용\n",
        "              optimizer=\"adam\",                         # 옵티마이저로 Adam 사용 - Adam은 경사 하강법의 한 종류, 모델의 가중치를 업데이트\n",
        "              metrics=[\"accuracy\"])                     # 모델 성능 평가 지표로 정확도(accuracy) 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- fit() 메서드를 사용하여 모델을 학습\n",
        "- X_train은 입력 데이터 y_train은 각 입력에 대한 레이블\n",
        "- 전체 데이터셋에 대해 모델을 몇 번 반복하여 학습할지를 결정하는 매개변수 epochs - 여기서는 20번의 에포크동안 학습\n",
        "- 모델의 성능을 평가하는데 사용될 검증 데이터셋 지정 validation_data=()\n",
        "    - 각 에포크 끝날 때 마다 검증 데이터셋에 대한 loss와 accuracy가 계산됨\n",
        "⬇️"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ-2WXRwGXFx",
        "outputId": "964989f8-856a-49bf-eebe-9497db9ed8a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "68/68 [==============================] - 2s 10ms/step - loss: 0.6696 - accuracy: 0.8277 - val_loss: 0.2311 - val_accuracy: 0.9338\n",
            "Epoch 2/20\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0850 - accuracy: 0.9733 - val_loss: 0.2183 - val_accuracy: 0.9191\n",
            "Epoch 3/20\n",
            "68/68 [==============================] - 1s 7ms/step - loss: 0.0196 - accuracy: 0.9972 - val_loss: 0.2271 - val_accuracy: 0.9301\n",
            "Epoch 4/20\n",
            "68/68 [==============================] - 0s 7ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.2319 - val_accuracy: 0.9412\n",
            "Epoch 5/20\n",
            "68/68 [==============================] - 1s 7ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.2433 - val_accuracy: 0.9375\n",
            "Epoch 6/20\n",
            "68/68 [==============================] - 1s 8ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.2508 - val_accuracy: 0.9375\n",
            "Epoch 7/20\n",
            "68/68 [==============================] - 0s 7ms/step - loss: 9.9387e-04 - accuracy: 1.0000 - val_loss: 0.2574 - val_accuracy: 0.9412\n",
            "Epoch 8/20\n",
            "68/68 [==============================] - 0s 7ms/step - loss: 7.5047e-04 - accuracy: 1.0000 - val_loss: 0.2633 - val_accuracy: 0.9412\n",
            "Epoch 9/20\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 5.8816e-04 - accuracy: 1.0000 - val_loss: 0.2694 - val_accuracy: 0.9412\n",
            "Epoch 10/20\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 4.7746e-04 - accuracy: 1.0000 - val_loss: 0.2743 - val_accuracy: 0.9412\n",
            "Epoch 11/20\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 3.9144e-04 - accuracy: 1.0000 - val_loss: 0.2791 - val_accuracy: 0.9375\n",
            "Epoch 12/20\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 3.2755e-04 - accuracy: 1.0000 - val_loss: 0.2834 - val_accuracy: 0.9375\n",
            "Epoch 13/20\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 2.7900e-04 - accuracy: 1.0000 - val_loss: 0.2873 - val_accuracy: 0.9375\n",
            "Epoch 14/20\n",
            "68/68 [==============================] - 0s 7ms/step - loss: 2.4044e-04 - accuracy: 1.0000 - val_loss: 0.2913 - val_accuracy: 0.9375\n",
            "Epoch 15/20\n",
            "68/68 [==============================] - 0s 7ms/step - loss: 2.0822e-04 - accuracy: 1.0000 - val_loss: 0.2947 - val_accuracy: 0.9375\n",
            "Epoch 16/20\n",
            "68/68 [==============================] - 0s 7ms/step - loss: 1.8256e-04 - accuracy: 1.0000 - val_loss: 0.2985 - val_accuracy: 0.9375\n",
            "Epoch 17/20\n",
            "68/68 [==============================] - 0s 7ms/step - loss: 1.6112e-04 - accuracy: 1.0000 - val_loss: 0.3018 - val_accuracy: 0.9375\n",
            "Epoch 18/20\n",
            "68/68 [==============================] - 0s 7ms/step - loss: 1.4321e-04 - accuracy: 1.0000 - val_loss: 0.3049 - val_accuracy: 0.9375\n",
            "Epoch 19/20\n",
            "68/68 [==============================] - 0s 7ms/step - loss: 1.2789e-04 - accuracy: 1.0000 - val_loss: 0.3080 - val_accuracy: 0.9375\n",
            "Epoch 20/20\n",
            "68/68 [==============================] - 1s 7ms/step - loss: 1.1512e-04 - accuracy: 1.0000 - val_loss: 0.3107 - val_accuracy: 0.9375\n"
          ]
        }
      ],
      "source": [
        " \n",
        "history = model.fit(X_train, y_train, epochs=20,         \n",
        "                    validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ct-BNp17GXDp",
        "outputId": "e3214867-d875-4df2-dcd5-ae5122d6e57f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 0s 4ms/step - loss: 0.2989 - accuracy: 0.9301\n"
          ]
        }
      ],
      "source": [
        "loss, acc = model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53ArfDZakYC_",
        "outputId": "1f9892ec-b7f6-427f-820c-86e643883fbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The loss for test data is 0.30, and the accuracy is 0.93\n"
          ]
        }
      ],
      "source": [
        "print(f'The loss for test data is {loss:.2f}, and the accuracy is {acc:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8yU6AJCJbQn"
      },
      "outputs": [],
      "source": [
        "#####################"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
